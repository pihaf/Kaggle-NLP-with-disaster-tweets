{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30527,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport re\nimport string\nimport operator\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\nfrom tensorflow.keras import layers, callbacks","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:25:26.799188Z","iopub.execute_input":"2024-03-08T07:25:26.799552Z","iopub.status.idle":"2024-03-08T07:25:26.807559Z","shell.execute_reply.started":"2024-03-08T07:25:26.799517Z","shell.execute_reply":"2024-03-08T07:25:26.806256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and explore the dataset\n\n## Load the train and test dataset ","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\nprint('Training Set Shape = {}'.format(df_train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\nprint('Test Set Shape = {}'.format(df_test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:25:31.743013Z","iopub.execute_input":"2024-03-08T07:25:31.743547Z","iopub.status.idle":"2024-03-08T07:25:31.803420Z","shell.execute_reply.started":"2024-03-08T07:25:31.743501Z","shell.execute_reply":"2024-03-08T07:25:31.801695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explore the dataset","metadata":{}},{"cell_type":"markdown","source":"Let's look at the first five rows of each dataset","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:25:34.907723Z","iopub.execute_input":"2024-03-08T07:25:34.908219Z","iopub.status.idle":"2024-03-08T07:25:34.932555Z","shell.execute_reply.started":"2024-03-08T07:25:34.908183Z","shell.execute_reply":"2024-03-08T07:25:34.930742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:25:37.800154Z","iopub.execute_input":"2024-03-08T07:25:37.800614Z","iopub.status.idle":"2024-03-08T07:25:37.814977Z","shell.execute_reply.started":"2024-03-08T07:25:37.800543Z","shell.execute_reply":"2024-03-08T07:25:37.813458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both datasets contain:\n- id\n- keyword: A keyword from that tweet (although this may be blank)\n- location: The location the tweet was sent from (may also be blank)\n- text: The text of a tweet\n\nWhile the train dataset has an extra column:\n- target: 1 if the tweet is a real disaster or 0 if not","metadata":{}},{"cell_type":"markdown","source":"# Preprocess the data","metadata":{}},{"cell_type":"markdown","source":"Let's take a look at the total number of missing values in two columns `keyword` and `location`.","metadata":{}},{"cell_type":"code","source":"print('Number of missing values of keyword column in train set {}'.format(df_train['keyword'].isnull().sum()))\nprint('Number of missing values of location column in train set {}'.format(df_train['location'].isnull().sum()))\nprint('Number of missing values of keyword column in test set {}'.format(df_test['keyword'].isnull().sum()))\nprint('Number of missing values of location column in test set {}'.format(df_test['location'].isnull().sum()))","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:25:42.575692Z","iopub.execute_input":"2024-03-08T07:25:42.576254Z","iopub.status.idle":"2024-03-08T07:25:42.592336Z","shell.execute_reply.started":"2024-03-08T07:25:42.576213Z","shell.execute_reply":"2024-03-08T07:25:42.589832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is a plot for better visualization","metadata":{}},{"cell_type":"code","source":"missing_cols = ['keyword', 'location']\n\nfig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)\n\nsns.barplot(x=df_train[missing_cols].isnull().sum().index, y=df_train[missing_cols].isnull().sum().values, ax=axes[0])\nsns.barplot(x=df_test[missing_cols].isnull().sum().index, y=df_test[missing_cols].isnull().sum().values, ax=axes[1])\n\naxes[0].set_ylabel('Missing Value Count', size=15, labelpad=20)\naxes[0].tick_params(axis='x', labelsize=15)\naxes[0].tick_params(axis='y', labelsize=15)\naxes[1].tick_params(axis='x', labelsize=15)\naxes[1].tick_params(axis='y', labelsize=15)\n\naxes[0].set_title('Training Set', fontsize=13)\naxes[1].set_title('Test Set', fontsize=13)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:25:45.550852Z","iopub.execute_input":"2024-03-08T07:25:45.551306Z","iopub.status.idle":"2024-03-08T07:25:46.048548Z","shell.execute_reply.started":"2024-03-08T07:25:45.551272Z","shell.execute_reply":"2024-03-08T07:25:46.047083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fill the missing values with `no_keyword` and `no_location` respectively.","metadata":{}},{"cell_type":"code","source":"for df in [df_train, df_test]:\n    for col in ['keyword', 'location']:\n        df[col] = df[col].fillna(f'no_{col}')","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:25:52.200773Z","iopub.execute_input":"2024-03-08T07:25:52.201319Z","iopub.status.idle":"2024-03-08T07:25:52.219691Z","shell.execute_reply.started":"2024-03-08T07:25:52.201278Z","shell.execute_reply":"2024-03-08T07:25:52.218037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since locations are user inputs, the `location` column can be very dirty and has a lot of unique values. Let's look at the number of unique values in both columns of the datasets.","metadata":{}},{"cell_type":"code","source":"print('Number of unique values of keyword column in train set {}'.format(df_train['keyword'].nunique()))\nprint('Number of unique values of location column in train set {}'.format(df_train['location'].nunique()))\nprint('Number of unique values of keyword column in test set {}'.format(df_test['keyword'].nunique()))\nprint('Number of unique values of location column in test set {}'.format(df_test['location'].nunique()))","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:25:55.033484Z","iopub.execute_input":"2024-03-08T07:25:55.033980Z","iopub.status.idle":"2024-03-08T07:25:55.049846Z","shell.execute_reply.started":"2024-03-08T07:25:55.033943Z","shell.execute_reply":"2024-03-08T07:25:55.048243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Every single keyword in training set exists in test set. If training and test set are from the same sample, it is possible to use target encoding on `keyword`.","metadata":{}},{"cell_type":"markdown","source":"Check for duplicates and remove if found","metadata":{}},{"cell_type":"code","source":"train_duplicates = df_train.duplicated()\ntest_duplicates = df_test.duplicated()\n\nprint('Total number of duplicates in train dataset: {}'.format(train_duplicates.sum()))\nprint('Rows with duplicates: \\n{}'.format(df_train[train_duplicates]))\nprint('Total number of duplicates in test dataset: {}'.format(test_duplicates.sum()))\nprint('Rows with duplicates: \\n{}'.format(df_test[test_duplicates]))","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:25:59.881652Z","iopub.execute_input":"2024-03-08T07:25:59.882099Z","iopub.status.idle":"2024-03-08T07:25:59.910792Z","shell.execute_reply.started":"2024-03-08T07:25:59.882063Z","shell.execute_reply":"2024-03-08T07:25:59.909452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Encode the `text` column","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\ntext_features = vectorizer.fit_transform(df_train['text'])\nX_test = vectorizer.transform(df_test[\"text\"]).toarray()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:26:03.640281Z","iopub.execute_input":"2024-03-08T07:26:03.640830Z","iopub.status.idle":"2024-03-08T07:26:04.577899Z","shell.execute_reply.started":"2024-03-08T07:26:03.640791Z","shell.execute_reply":"2024-03-08T07:26:04.576552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the train dataset to a train and a validation set","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nX_train, X_val, y_train, y_val = train_test_split(text_features, df_train['target'], test_size=0.2, random_state=42)\n\nX_train = X_train.toarray()\nX_val = X_val.toarray()\n\n# Get the input shape\ninput_shape = X_train.shape[1]","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:26:22.520938Z","iopub.execute_input":"2024-03-08T07:26:22.521421Z","iopub.status.idle":"2024-03-08T07:26:23.109479Z","shell.execute_reply.started":"2024-03-08T07:26:22.521380Z","shell.execute_reply":"2024-03-08T07:26:23.108067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and train model","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\nNUM_TRAINING_EXAMPLES = df_train.shape[0]\n\nEPOCHS = 30\nAUTO = tf.data.experimental.AUTOTUNE\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, \n    patience=10, \n    restore_best_weights=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:30:49.739779Z","iopub.execute_input":"2024-03-08T07:30:49.740285Z","iopub.status.idle":"2024-03-08T07:30:49.747823Z","shell.execute_reply.started":"2024-03-08T07:30:49.740249Z","shell.execute_reply":"2024-03-08T07:30:49.746136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a sequential model","metadata":{}},{"cell_type":"code","source":"model = keras.Sequential([\n    layers.Dense(256, activation='relu', input_shape=(input_shape,)),\n    layers.BatchNormalization(),\n    layers.Dropout(rate=0.3),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(rate=0.3),\n    layers.Dense(1, activation='sigmoid')\n])","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:30:53.774231Z","iopub.execute_input":"2024-03-08T07:30:53.774733Z","iopub.status.idle":"2024-03-08T07:30:53.942959Z","shell.execute_reply.started":"2024-03-08T07:30:53.774696Z","shell.execute_reply":"2024-03-08T07:30:53.941219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)\n\n# Fit\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=[early_stopping],\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:30:59.107349Z","iopub.execute_input":"2024-03-08T07:30:59.108478Z","iopub.status.idle":"2024-03-08T07:32:54.293561Z","shell.execute_reply.started":"2024-03-08T07:30:59.108420Z","shell.execute_reply":"2024-03-08T07:32:54.292084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_train = model.predict(X_train)\ny_pred_train = np.where(y_pred_train > 0.5, 1, 0)\n\nprint(\"Classification report of training: \\n\", classification_report(y_train, y_pred_train))","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:33:07.257059Z","iopub.execute_input":"2024-03-08T07:33:07.257566Z","iopub.status.idle":"2024-03-08T07:33:09.952942Z","shell.execute_reply.started":"2024-03-08T07:33:07.257526Z","shell.execute_reply":"2024-03-08T07:33:09.951647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_val = model.predict(X_val)\ny_pred_val = np.where(y_pred_val > 0.5, 1, 0)\n\nprint(\"Classification report of validation: \\n\", classification_report(y_val, y_pred_val))","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:33:14.538162Z","iopub.execute_input":"2024-03-08T07:33:14.539110Z","iopub.status.idle":"2024-03-08T07:33:15.416261Z","shell.execute_reply.started":"2024-03-08T07:33:14.539042Z","shell.execute_reply":"2024-03-08T07:33:15.414553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate the submission file \n\nFor each tweets in the test set, we predict if the given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n\nThe `submission.csv` file uses the following format:\n`id,target`","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:33:26.412099Z","iopub.execute_input":"2024-03-08T07:33:26.413391Z","iopub.status.idle":"2024-03-08T07:33:26.429594Z","shell.execute_reply.started":"2024-03-08T07:33:26.413326Z","shell.execute_reply":"2024-03-08T07:33:26.428448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = model.predict(X_test)\ny_test = np.where(y_test > 0.5, 1, 0)\n\nsubmission[\"target\"] = y_test\nprint(submission)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:40:48.297622Z","iopub.execute_input":"2024-03-08T07:40:48.298512Z","iopub.status.idle":"2024-03-08T07:40:50.010669Z","shell.execute_reply.started":"2024-03-08T07:40:48.298297Z","shell.execute_reply":"2024-03-08T07:40:50.009185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.describe()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:40:55.923379Z","iopub.execute_input":"2024-03-08T07:40:55.923821Z","iopub.status.idle":"2024-03-08T07:40:55.945077Z","shell.execute_reply.started":"2024-03-08T07:40:55.923786Z","shell.execute_reply":"2024-03-08T07:40:55.943692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:41:04.516105Z","iopub.execute_input":"2024-03-08T07:41:04.516561Z","iopub.status.idle":"2024-03-08T07:41:04.536350Z","shell.execute_reply.started":"2024-03-08T07:41:04.516524Z","shell.execute_reply":"2024-03-08T07:41:04.534671Z"},"trusted":true},"execution_count":null,"outputs":[]}]}